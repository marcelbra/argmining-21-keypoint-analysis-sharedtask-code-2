{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "- Import model\n",
    "    Import the trained model instead of training from scratch\n",
    "    - See cell 3\n",
    "    - The basic idea is:\n",
    "        - initialize the model as it was trained, in sbert_training the definition can be found\n",
    "        - load the weights into the model (by whichever means)\n",
    "        - The trained model is here: https://drive.google.com/drive/folders/1qgGdoNMUcyQivTtu5udzGcQB8SFgxm-M?usp=sharing\n",
    "\n",
    "- Leave-one-out\n",
    "    Extend the method / dataloader with all instead of only the dummy sample\n",
    "    - The dev data can be found in /new_KPA/argmining-21-keypoint-analysis-sharedtask-code/data/kpm_data\n",
    "    - There could be two ways to approach this:\n",
    "        - Interpret the gold standard:\n",
    "            - Iterate over kps in key_points_dev.csv and grab the respective argument from arguments_dev.csv\n",
    "        - Let the model decide what is the right arg\n",
    "            - Iterate over all args, computer score with each kp and save argmax\n",
    "    - What needs to be added is for each InputExample save the word we dropped so we can access it during\n",
    "      entailment computing. For this copy InputExample from SentenceBert and modify class, save word to later access\n",
    "\n",
    "- SHAP\n",
    "    - TODO\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import *\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "sys.path.insert(0, \"/home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code/code/src-py\")\n",
    "import sbert_training\n",
    "work_tokenizer = word_tokenize\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 18:44:23 - Use pytorch device: cuda\n",
      "2022-01-19 18:44:23 - Read Triplet train dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36cf5face69242be9f93c34e228e7b38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/645 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ffdb3176ba94a348fea7203bbf8aed4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 18:45:10 - TripletEvaluator: Evaluating the model on dev dataset in epoch 0 after 500 steps:\n",
      "mAP strict= 0.7786461511627489 ; mAP relaxed = 0.9703035170419233\n",
      "2022-01-19 18:45:11 - mAP strict:   \t77.86\n",
      "2022-01-19 18:45:11 - mAP relaxed:   \t97.03\n",
      "2022-01-19 18:45:11 - Save model to /home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-modelsroberta-base-contrastive-10-epochs-2022-01-19_18-44-14\n",
      "2022-01-19 18:45:24 - TripletEvaluator: Evaluating the model on dev dataset after epoch 0:\n",
      "mAP strict= 0.7955930287423483 ; mAP relaxed = 0.9735898568399985\n",
      "2022-01-19 18:45:25 - mAP strict:   \t79.56\n",
      "2022-01-19 18:45:25 - mAP relaxed:   \t97.36\n",
      "2022-01-19 18:45:25 - Save model to /home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-modelsroberta-base-contrastive-10-epochs-2022-01-19_18-44-14\n"
     ]
    }
   ],
   "source": [
    "model = sbert_training.train_model('/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/siamese-data/',\n",
    "                           \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/kpm_data\",\n",
    "                           'dev',\n",
    "                           \"/home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-models\",\n",
    "                           'roberta-base',\n",
    "                           model_suffix='contrastive-10-epochs',\n",
    "                           data_file_suffix='contrastive',\n",
    "                           num_epochs=1, max_seq_length=70, add_special_token=True, train_batch_size=32, loss='ContrastiveLoss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# def load_model():\n",
    "#     max_seq_length = 70\n",
    "#     model_name = 'distilbert-base-uncased'\n",
    "#     word_embedding_model = models.Transformer(model_name)\n",
    "#     word_embedding_model.max_seq_length = max_seq_length\n",
    "#     word_embedding_model.tokenizer.add_tokens(['<SEP>'], special_tokens=True)\n",
    "#     word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n",
    "#     pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "#                                    pooling_mode_mean_tokens=True,\n",
    "#                                    pooling_mode_cls_token=False,\n",
    "#                                    pooling_mode_max_tokens=False)\n",
    "#     model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "#     return model\n",
    "\n",
    "# config = BertConfig.from_json_file(\"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model/config.json\")\n",
    "# model = TFBertModel.from_pretrained('/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model/my_pytorch_model.bin', from_pt=True, config=config)\n",
    "\n",
    "# path = \"./siamese_state_dict_2.pt\"\n",
    "# state_dict = torch.load(path)[\"state_dict\"]\n",
    "# model = load_model()\n",
    "# model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "\n",
    "    def __init__(self,\n",
    "                 arg: str = None,\n",
    "                 kp: str = None,\n",
    "                 label: Union[int, float] = 0,\n",
    "                 dropped_word: str = None\n",
    "                 ):\n",
    "\n",
    "        self.arg = arg\n",
    "        self.kp = kp\n",
    "        self.label = label\n",
    "        self.dropped_word = dropped_word\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<InputExample> label: {}, texts: {}\".format(str(self.label), \"; \".join(self.texts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def get_data_for_leave_one_out():\n",
    "    \"\"\"\n",
    "    Creates a nested list of data.\n",
    "    For each arg-kp pair we create no_of_words_in_kp InputExamples.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    # Here all the data should be loaded instead of only 1 test sample\n",
    "    for _ in range(1):\n",
    "        topic = \"We should abandon the use of school uniform\"\n",
    "        arg = \"A real education is about giving students the tools to learn, think, and express themselves; dictating to them what to wear sends a strong message that we don't trust them to think on their own.\"\n",
    "        kp = \"School uniform is harming the student's self expression\"\n",
    "        label = 1\n",
    "        sample = [InputExample(arg=arg,\n",
    "                               kp= kp + \" <SEP> \" + topic,\n",
    "                               label=label,\n",
    "                               dropped_word=\"Reference\")]\n",
    "        words = work_tokenizer(kp)\n",
    "        for i in range(len(words)):\n",
    "            new_kp = copy.deepcopy(words)\n",
    "            new_kp.pop(i)\n",
    "            new_kp_topic = \" \".join(new_kp)\n",
    "            dropped_word = words[i]\n",
    "            sample.append(InputExample(arg=arg,\n",
    "                                       kp=new_kp_topic,\n",
    "                                       label=label,\n",
    "                                       dropped_word=dropped_word))\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# def get_dataloaders_for_leave_one_out(data, model):\n",
    "#     dataloaders = []\n",
    "#     for sample in data:\n",
    "#         # sample has no_of_words_in_kp examples in it\n",
    "#         dataloader = DataLoader(sample, shuffle=False, batch_size=1)\n",
    "#         #dataloader.collate_fn = model.smart_batching_collate\n",
    "#         dataloaders.append(dataloader)\n",
    "#     return dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def compute_entailment(arg, kp):\n",
    "    return util.pytorch_cos_sim(arg, kp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f329336b67e14255afad9f140ed212d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7952/2484059171.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_data_for_leave_one_out\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mleave_one_out\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamples\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"\\n\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_7952/2484059171.py\u001B[0m in \u001B[0;36mleave_one_out\u001B[0;34m(model, data)\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodified_sentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m             r = {\"dropped_word\": modified_sentence.dropped_word,\n\u001B[0m\u001B[1;32m      7\u001B[0m                  \"score\": float(compute_entailment(model.encode(modified_sentence.arg),\n\u001B[1;32m      8\u001B[0m                                                    model.encode(modified_sentence.kp)))}\n",
      "\u001B[0;32m/tmp/ipykernel_7952/2484059171.py\u001B[0m in \u001B[0;36mleave_one_out\u001B[0;34m(model, data)\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodified_sentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m             r = {\"dropped_word\": modified_sentence.dropped_word,\n\u001B[0m\u001B[1;32m      7\u001B[0m                  \"score\": float(compute_entailment(model.encode(modified_sentence.arg),\n\u001B[1;32m      8\u001B[0m                                                    model.encode(modified_sentence.kp)))}\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def leave_one_out(model, data):\n",
    "    results = []\n",
    "    for sentence in samples:\n",
    "        result = []\n",
    "        for i, modified_sentence in enumerate(sentence):\n",
    "            r = {\"dropped_word\": modified_sentence.dropped_word,\n",
    "                 \"score\": float(compute_entailment(model.encode(modified_sentence.arg),\n",
    "                                                   model.encode(modified_sentence.kp)))}\n",
    "            result.append(r)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "samples = get_data_for_leave_one_out()\n",
    "results = leave_one_out(model, samples)\n",
    "print(*(results[0]), sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "################# CODE GRAVEYARD ###########################\n",
    "############################################################\n",
    "############################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# modified_features = [{\"input_ids\": features[0][\"input_ids\"],\n",
    "#                       \"attention_mask\": features[0][\"attention_mask\"]},\n",
    "#                      {\"input_ids\": features[1][\"input_ids\"],\n",
    "#                       \"attention_mask\": features[1][\"attention_mask\"]}]\n",
    "\n",
    "# Prepare tokens\n",
    "# kp_topic = modified_features[1][\"input_ids\"]\n",
    "# kp_topic_tokens = modified_features[1][\"input_ids\"].tolist()[0]\n",
    "# sep_token_index = kp_topic_tokens.index(50265) # <SEP> Token\n",
    "# kp_tokens = kp_topic_tokens[:sep_token_index]\n",
    "# topic_tokens = kp_topic_tokens[sep_token_index:]\n",
    "\n",
    "# Prepare list of kps where a token is dropped\n",
    "# new_kps = []\n",
    "# for i in range(len(kp_tokens)):\n",
    "#     dropped = kp_tokens[i]\n",
    "#     new_kp = copy.deepcopy(kp_tokens)\n",
    "#     new_kp.pop(i)\n",
    "#     new_kps.append({\"kp\": new_kp, \"dropped\": dropped, \"index\": i})\n",
    "\n",
    "# # Create dropped modified_features\n",
    "# for new_kp in new_kps:\n",
    "#     kp, dropped, index = new_kp.values()\n",
    "#\n",
    "#     # Modify sample itself\n",
    "#     modified_features[1][\"input_ids\"] = torch.tensor(kp+topic_tokens)\n",
    "#\n",
    "#     # Modify attention mask\n",
    "#     old_attention_mask = modified_features[1][\"attention_mask\"][0]\n",
    "#     first_part = old_attention_mask[:index]\n",
    "#     second_part = old_attention_mask[index+1:] # Here dropped index is excluded\n",
    "#     new_attention_mask = torch.cat([first_part, second_part], dim=0)\n",
    "#     new_attention_mask = new_attention_mask.view(1,len(new_attention_mask))\n",
    "#     modified_features[1][\"attention_mask\"] = new_attention_mask\n",
    "#\n",
    "#     dropped_score = compute_entailment(loss_model, modified_features, labels)\n",
    "#\n",
    "#     dropped_scores.append({\"score\": dropped_score,\n",
    "#                            \"dropped\": dropped,\n",
    "#                            \"index\": index})\n",
    "#     s = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def get_backbone_model_and_tokenizer():\n",
    "#     name = 'roberta-base'\n",
    "#     backbone = models.Transformer(name)\n",
    "#     backbone.max_seq_length = 70\n",
    "#     backbone.tokenizer.add_tokens(['<SEP>'], special_tokens=True)\n",
    "#     backbone.auto_model.resize_token_embeddings(len(backbone.tokenizer))\n",
    "#     return backbone, backbone.tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #Modify token embeddings\n",
    "# old_token_embedding = features[1][\"token_embeddings\"][0]\n",
    "# first_part = features[1][\"token_embeddings\"][0][:index]\n",
    "# second_part = features[1][\"token_embeddings\"][0][index+1:]\n",
    "# new_token_embedding = torch.cat([first_part, second_part])\n",
    "# new_token_embedding = new_token_embedding.view(1, *new_token_embedding.size())\n",
    "# features[1][\"token_embeddings\"] = new_token_embedding\n",
    "\n",
    "# Modify sentence embeddings\n",
    "# We can just drop the sentence embeddings, the model will infer this automatically"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# def get_dataloader(examples, model):\n",
    "#     dataloader = DataLoader(examples, shuffle=False, batch_size=1)\n",
    "#     dataloader.collate_fn = model.smart_batching_collate\n",
    "#     return iter(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def prepare_model_for_inference(model):\n",
    "    model = losses.ContrastiveLoss(model)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def compute_entailment(model, features, labels):\n",
    "#     return float((1 - model(features, labels)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}