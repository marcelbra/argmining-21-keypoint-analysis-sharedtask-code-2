{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import *\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "sys.path.insert(0, \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/code/src-py\")\n",
    "import sbert_training\n",
    "work_tokenizer = word_tokenize\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "repo_dir = \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/\"\n",
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    path = \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model\"\n",
    "    model = SentenceTransformer(path)\n",
    "except:\n",
    "    model = sbert_training.train_model('/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/siamese-data/',\n",
    "                                       \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/kpm_data\",\n",
    "                                       'dev',\n",
    "                                       \"/home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-models\",\n",
    "                                       'roberta-base',\n",
    "                                       model_suffix='contrastive-10-epochs',\n",
    "                                       data_file_suffix='contrastive',\n",
    "                                      num_epochs=1, max_seq_length=70, add_special_token=True, train_batch_size=32, loss='ContrastiveLoss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_arg_kps_mapping(arguments_df, key_points_df):\n",
    "    mapping = {}\n",
    "    topics = arguments_df[\"topic\"].unique()\n",
    "    for topic in topics:\n",
    "        arguments = arguments_df.loc[arguments_df[\"topic\"] == topic][[\"argument\"]].drop_duplicates()\n",
    "        key_points = key_points_df.loc[key_points_df[\"topic\"] == topic][[\"key_point\"]].drop_duplicates()\n",
    "        map = pd.merge(arguments, key_points, how=\"cross\")\n",
    "        mapping[topic] = map\n",
    "    return mapping\n",
    "\n",
    "def load_kpm_data():\n",
    "\n",
    "    try:\n",
    "        with open(\"gold_labels_and_prediction_scores.pkl\", \"rb\") as handle:\n",
    "            return pickle.load(handle)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    data = defaultdict(dict)\n",
    "    for subset in [\"dev\"]:#, \"train\"]:\n",
    "        # Load files\n",
    "        arguments_file = repo_dir + f\"data/kpm_data/arguments_{subset}.csv\"\n",
    "        key_points_file = repo_dir + f\"data/kpm_data/key_points_{subset}.csv\"\n",
    "        labels_file = repo_dir + f\"data/kpm_data/labels_{subset}.csv\"\n",
    "        arguments_df = pd.read_csv(arguments_file)\n",
    "        key_points_df = pd.read_csv(key_points_file)\n",
    "        labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "        # Get gold standard\n",
    "        positive_labels_df = labels_df.loc[labels_df[\"label\"] == 1]\n",
    "        gold_standard = pd.merge(positive_labels_df, key_points_df, how=\"inner\", on=\"key_point_id\")\n",
    "        gold_standard = pd.merge(gold_standard, arguments_df, how=\"inner\", on=[\"arg_id\",\"topic\"])\n",
    "        gold_standard = gold_standard[[\"topic\", \"argument\", \"key_point\"]]\n",
    "        gold_standard[\"score\"] = 1\n",
    "        data[subset][\"gold_standard\"] = gold_standard\n",
    "\n",
    "        # Compute model scores\n",
    "        def compute_score_from(row):\n",
    "            argument = row[\"argument\"]\n",
    "            key_point = row[\"key_point\"]\n",
    "            return compute_entailment_from_arg_kp(argument, key_point, model)\n",
    "\n",
    "        mappings = []\n",
    "        arg_to_kps = create_arg_kps_mapping(arguments_df, key_points_df)\n",
    "        for topic, arg_kps_mapping in arg_to_kps.items():\n",
    "            arg_kps_mapping['score'] = arg_kps_mapping.apply(lambda row: compute_score_from(row), axis=1)\n",
    "            arg_kps_mapping['topic'] = topic\n",
    "            arg_kps_mapping = arg_kps_mapping[[\"topic\", \"argument\", \"key_point\", \"score\"]]\n",
    "            mappings.append(arg_kps_mapping)\n",
    "        predictions = pd.concat(mappings, axis=0)\n",
    "        data[subset][\"predictions\"] = predictions\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "\n",
    "    def __init__(self,\n",
    "                 arg: str = None,\n",
    "                 kp: str = None,\n",
    "                 label: Union[int, float] = 0,\n",
    "                 dropped_word: str = None\n",
    "                 ):\n",
    "\n",
    "        self.arg = arg\n",
    "        self.kp = kp\n",
    "        self.dropped_word = dropped_word\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<InputExample> label: {}, texts: {}\".format(str(self.label), \"; \".join(self.texts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7246/80950174.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0msamples\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 52\u001B[0;31m \u001B[0mget_data_for_leave_one_out\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_7246/80950174.py\u001B[0m in \u001B[0;36mget_data_for_leave_one_out\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdfs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"dev\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"predictions\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"key_point_words\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtokenize_kp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"dropped_sentences\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mcreate_leave_one_out_sents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0;31m# data[\"test\"] = Test(1)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_7246/80950174.py\u001B[0m in \u001B[0;36mget_data_for_leave_one_out\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdfs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"dev\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"predictions\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"key_point_words\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtokenize_kp\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mdata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"dropped_sentences\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mcreate_leave_one_out_sents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0;31m# data[\"test\"] = Test(1)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(py_db, frame, event, arg)\u001B[0m\n\u001B[1;32m    206\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mapply_to_settrace\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m         \u001B[0mpy_db\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_tracing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_trace_func\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mthread_trace_func\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    464\u001B[0m             \u001B[0;31m# Just create PyDBFrame directly (removed support for Python versions < 2.5, which required keeping a weak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    465\u001B[0m             \u001B[0;31m# reference to the frame).\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 466\u001B[0;31m             ret = PyDBFrame(\n\u001B[0m\u001B[1;32m    467\u001B[0m                 (\n\u001B[1;32m    468\u001B[0m                     \u001B[0mpy_db\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0madditional_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe_skips_cache\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe_cache_key\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def tokenize_kp(row):\n",
    "    return work_tokenizer(row[\"key_point\"])\n",
    "\n",
    "def create_leave_one_out_sents(row):\n",
    "    words = row[\"key_point_words\"]\n",
    "    samples = [{\"dropped\": \"Reference\", \"new_kp\": row}]\n",
    "    for i in range(len(words)):\n",
    "        new_kp = copy.deepcopy(words)\n",
    "        dropped_word = new_kp.pop(i)\n",
    "        new_kp = \" \".join(new_kp)\n",
    "        samples.append({\"dropped\": dropped_word, \"new_kp\": new_kp})\n",
    "    return samples\n",
    "\n",
    "\n",
    "def get_data_for_leave_one_out():\n",
    "    \"\"\"\n",
    "    Creates a nested list of data.\n",
    "    For each arg-kp pair we create no_of_words_in_kp InputExamples.\n",
    "    \"\"\"\n",
    "    dfs = load_kpm_data()\n",
    "    data = dfs[\"dev\"][\"predictions\"]\n",
    "    data[\"key_point_words\"] = data.apply(lambda row: tokenize_kp(row), axis=1)\n",
    "    data[\"dropped_sentences\"] = data.apply(lambda row: create_leave_one_out_sents(row), axis=1)\n",
    "    return data\n",
    "\n",
    "get_data_for_leave_one_out()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def compute_entailment(example, model):\n",
    "    arg = model.encode(example.arg, show_progress_bar=False),\n",
    "    kp = model.encode(example.kp, show_progress_bar=False)\n",
    "    return float(util.pytorch_cos_sim(arg, kp))\n",
    "\n",
    "def compute_entailment_from_arg_kp(arg, kp, model):\n",
    "    arg = model.encode(arg, show_progress_bar=False),\n",
    "    kp = model.encode(kp, show_progress_bar=False)\n",
    "    return float(util.pytorch_cos_sim(arg, kp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropped_word': 'Reference', 'score': 0.635991632938385}\n",
      "{'dropped_word': 'School', 'score': 0.6456453800201416}\n",
      "{'dropped_word': 'uniform', 'score': 0.6269412636756897}\n",
      "{'dropped_word': 'is', 'score': 0.6195348501205444}\n",
      "{'dropped_word': 'harming', 'score': 0.551324725151062}\n",
      "{'dropped_word': 'the', 'score': 0.6279229521751404}\n",
      "{'dropped_word': 'student', 'score': 0.6066310405731201}\n",
      "{'dropped_word': \"'s\", 'score': 0.6129819750785828}\n",
      "{'dropped_word': 'self', 'score': 0.5350502729415894}\n",
      "{'dropped_word': 'expression', 'score': 0.5559728145599365}\n"
     ]
    }
   ],
   "source": [
    "def leave_one_out(model, data):\n",
    "    results = []\n",
    "    for sentences in samples:\n",
    "        result = []\n",
    "        for i, example in enumerate(sentences):\n",
    "            r = {\"dropped_word\": example.dropped_word,\n",
    "                 \"score\": compute_entailment(example, model)}\n",
    "            result.append(r)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "samples = get_data_for_leave_one_out()\n",
    "results = leave_one_out(model, samples)\n",
    "print(*(results[0]), sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "################# CODE GRAVEYARD ###########################\n",
    "############################################################\n",
    "############################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# modified_features = [{\"input_ids\": features[0][\"input_ids\"],\n",
    "#                       \"attention_mask\": features[0][\"attention_mask\"]},\n",
    "#                      {\"input_ids\": features[1][\"input_ids\"],\n",
    "#                       \"attention_mask\": features[1][\"attention_mask\"]}]\n",
    "\n",
    "# Prepare tokens\n",
    "# kp_topic = modified_features[1][\"input_ids\"]\n",
    "# kp_topic_tokens = modified_features[1][\"input_ids\"].tolist()[0]\n",
    "# sep_token_index = kp_topic_tokens.index(50265) # <SEP> Token\n",
    "# kp_tokens = kp_topic_tokens[:sep_token_index]\n",
    "# topic_tokens = kp_topic_tokens[sep_token_index:]\n",
    "\n",
    "# Prepare list of kps where a token is dropped\n",
    "# new_kps = []\n",
    "# for i in range(len(kp_tokens)):\n",
    "#     dropped = kp_tokens[i]\n",
    "#     new_kp = copy.deepcopy(kp_tokens)\n",
    "#     new_kp.pop(i)\n",
    "#     new_kps.append({\"kp\": new_kp, \"dropped\": dropped, \"index\": i})\n",
    "\n",
    "# # Create dropped modified_features\n",
    "# for new_kp in new_kps:\n",
    "#     kp, dropped, index = new_kp.values()\n",
    "#\n",
    "#     # Modify sample itself\n",
    "#     modified_features[1][\"input_ids\"] = torch.tensor(kp+topic_tokens)\n",
    "#\n",
    "#     # Modify attention mask\n",
    "#     old_attention_mask = modified_features[1][\"attention_mask\"][0]\n",
    "#     first_part = old_attention_mask[:index]\n",
    "#     second_part = old_attention_mask[index+1:] # Here dropped index is excluded\n",
    "#     new_attention_mask = torch.cat([first_part, second_part], dim=0)\n",
    "#     new_attention_mask = new_attention_mask.view(1,len(new_attention_mask))\n",
    "#     modified_features[1][\"attention_mask\"] = new_attention_mask\n",
    "#\n",
    "#     dropped_score = compute_entailment(loss_model, modified_features, labels)\n",
    "#\n",
    "#     dropped_scores.append({\"score\": dropped_score,\n",
    "#                            \"dropped\": dropped,\n",
    "#                            \"index\": index})\n",
    "#     s = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# def get_backbone_model_and_tokenizer():\n",
    "#     name = 'roberta-base'\n",
    "#     backbone = models.Transformer(name)\n",
    "#     backbone.max_seq_length = 70\n",
    "#     backbone.tokenizer.add_tokens(['<SEP>'], special_tokens=True)\n",
    "#     backbone.auto_model.resize_token_embeddings(len(backbone.tokenizer))\n",
    "#     return backbone, backbone.tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# #Modify token embeddings\n",
    "# old_token_embedding = features[1][\"token_embeddings\"][0]\n",
    "# first_part = features[1][\"token_embeddings\"][0][:index]\n",
    "# second_part = features[1][\"token_embeddings\"][0][index+1:]\n",
    "# new_token_embedding = torch.cat([first_part, second_part])\n",
    "# new_token_embedding = new_token_embedding.view(1, *new_token_embedding.size())\n",
    "# features[1][\"token_embeddings\"] = new_token_embedding\n",
    "\n",
    "# Modify sentence embeddings\n",
    "# We can just drop the sentence embeddings, the model will infer this automatically"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# def get_dataloader(examples, model):\n",
    "#     dataloader = DataLoader(examples, shuffle=False, batch_size=1)\n",
    "#     dataloader.collate_fn = model.smart_batching_collate\n",
    "#     return iter(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# def get_dataloaders_for_leave_one_out(data, model):\n",
    "#     dataloaders = []\n",
    "#     for sample in data:\n",
    "#         # sample has no_of_words_in_kp examples in it\n",
    "#         dataloader = DataLoader(sample, shuffle=False, batch_size=1)\n",
    "#         #dataloader.collate_fn = model.smart_batching_collate\n",
    "#         dataloaders.append(dataloader)\n",
    "#     return dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# def prepare_model_for_inference(model):\n",
    "#     model = losses.ContrastiveLoss(model)\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# def compute_entailment(model, features, labels):\n",
    "#     return float((1 - model(features, labels)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}