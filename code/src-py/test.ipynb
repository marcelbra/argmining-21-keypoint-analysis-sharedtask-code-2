{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "import sbert_training\n",
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "import sys\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "word_tokenizer = word_tokenize\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "repo_dir = \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    try:\n",
    "        path = \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model\"\n",
    "        model = SentenceTransformer(path)\n",
    "    except:\n",
    "        model = sbert_training.train_model('/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/siamese-data/',\n",
    "                                           \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/kpm_data\",\n",
    "                                           'dev',\n",
    "                                           \"/home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-models\",\n",
    "                                           'roberta-base',\n",
    "                                           model_suffix='contrastive-10-epochs',\n",
    "                                           data_file_suffix='contrastive',\n",
    "                                          num_epochs=1, max_seq_length=70, add_special_token=True, train_batch_size=32, loss='ContrastiveLoss')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 15:44:23 - Load pretrained SentenceTransformer: /home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model\n",
      "2022-01-20 15:44:24 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = load_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_arg_kps_mapping(arguments_df, key_points_df):\n",
    "    mapping = {}\n",
    "    topics = arguments_df[\"topic\"].unique()\n",
    "    for topic in topics:\n",
    "        arguments = arguments_df.loc[arguments_df[\"topic\"] == topic][[\"argument\"]].drop_duplicates()\n",
    "        key_points = key_points_df.loc[key_points_df[\"topic\"] == topic][[\"key_point\"]].drop_duplicates()\n",
    "        map = pd.merge(arguments, key_points, how=\"cross\")\n",
    "        mapping[topic] = map\n",
    "    return mapping\n",
    "\n",
    "def load_kpm_data():\n",
    "\n",
    "    try:\n",
    "        with open(\"gold_labels_and_prediction_scores.pkl\", \"rb\") as handle:\n",
    "            return pickle.load(handle)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    data = defaultdict(dict)\n",
    "    for subset in [\"dev\"]:#, \"train\"]:\n",
    "        # Load files\n",
    "        arguments_file = repo_dir + f\"data/kpm_data/arguments_{subset}.csv\"\n",
    "        key_points_file = repo_dir + f\"data/kpm_data/key_points_{subset}.csv\"\n",
    "        labels_file = repo_dir + f\"data/kpm_data/labels_{subset}.csv\"\n",
    "        arguments_df = pd.read_csv(arguments_file)\n",
    "        key_points_df = pd.read_csv(key_points_file)\n",
    "        labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "        # Get gold standard\n",
    "        positive_labels_df = labels_df.loc[labels_df[\"label\"] == 1]\n",
    "        gold_standard = pd.merge(positive_labels_df, key_points_df, how=\"inner\", on=\"key_point_id\")\n",
    "        gold_standard = pd.merge(gold_standard, arguments_df, how=\"inner\", on=[\"arg_id\",\"topic\"])\n",
    "        gold_standard = gold_standard[[\"topic\", \"argument\", \"key_point\"]]\n",
    "        gold_standard[\"score\"] = 1\n",
    "        data[subset][\"gold_standard\"] = gold_standard\n",
    "\n",
    "        # Compute model scores\n",
    "        def compute_score_from(row):\n",
    "            argument = row[\"argument\"]\n",
    "            key_point = row[\"key_point\"]\n",
    "            return compute_entailment(argument, key_point, model)\n",
    "\n",
    "        mappings = []\n",
    "        arg_to_kps = create_arg_kps_mapping(arguments_df, key_points_df)\n",
    "        for topic, arg_kps_mapping in arg_to_kps.items():\n",
    "            arg_kps_mapping['score'] = arg_kps_mapping.apply(lambda row: compute_score_from(row), axis=1)\n",
    "            arg_kps_mapping['topic'] = topic\n",
    "            arg_kps_mapping = arg_kps_mapping[[\"topic\", \"argument\", \"key_point\", \"score\"]]\n",
    "            mappings.append(arg_kps_mapping)\n",
    "        predictions = pd.concat(mappings, axis=0)\n",
    "        data[subset][\"predictions\"] = predictions\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def compute_entailment(arg, kp, model):\n",
    "    arg = model.encode(arg, show_progress_bar=False),\n",
    "    kp = model.encode(kp, show_progress_bar=False)\n",
    "    return float(util.pytorch_cos_sim(arg, kp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def tokenize_kp(row):\n",
    "    return word_tokenizer(row[\"key_point\"])\n",
    "\n",
    "def run_leave_one_out(row):\n",
    "    words = row[\"key_point_words\"]\n",
    "    samples = [{\"dropped\": \"Reference\", \"new_kp\": row[\"key_point\"], \"score\": row[\"score\"]}]\n",
    "    for i in range(len(words)):\n",
    "        new_kp = copy.deepcopy(words)\n",
    "        dropped_word = new_kp.pop(i)\n",
    "        new_kp = \" \".join(new_kp)\n",
    "        new_score = compute_entailment(row[\"argument\"], new_kp)\n",
    "        samples.append({\"dropped\": dropped_word, \"new_kp\": new_kp, \"score\": new_score})\n",
    "    return samples\n",
    "\n",
    "def get_data_for_leave_one_out():\n",
    "    dfs = load_kpm_data()\n",
    "    data = dfs[\"dev\"][\"predictions\"]\n",
    "    data[\"key_point_words\"] = data.apply(lambda row: tokenize_kp(row), axis=1)\n",
    "    data[\"leave_one_out\"] = data.apply(lambda row: run_leave_one_out(row), axis=1)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def save_with_pickle(path, data):\n",
    "    with open(path, \"wb\") as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "save_with_pickle(\"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/code/src-py/test_.pkl\", [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}