{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nTODO:\\n\\n- Import model\\n    Import the trained model instead of training from scratch\\n    - See cell 3\\n    - The basic idea is:\\n        - initialize the model as it was trained, in sbert_training the definition can be found\\n        - load the weights into the model (by whichever means)\\n        - The trained model is here: https://drive.google.com/drive/folders/1qgGdoNMUcyQivTtu5udzGcQB8SFgxm-M?usp=sharing\\n\\n- Leave-one-out\\n    - There could be two ways to approach this:\\n        - Interpret the gold standard:\\n            - Iterate over kps in key_points_dev.csv and grab the respective argument from arguments_dev.csv\\n        - Let the model decide what is the right arg\\n            - Iterate over all args, computer score with each kp and save argmax\\n    - What needs to be added is for each InputExample save the word we dropped so we can access it during\\n      entailment computing. For this copy InputExample from SentenceBert and modify class, save word to later access\\n\\n- SHAP\\n    - TODO\\n'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "- Import model\n",
    "    Import the trained model instead of training from scratch\n",
    "    - See cell 3\n",
    "    - The basic idea is:\n",
    "        - initialize the model as it was trained, in sbert_training the definition can be found\n",
    "        - load the weights into the model (by whichever means)\n",
    "        - The trained model is here: https://drive.google.com/drive/folders/1qgGdoNMUcyQivTtu5udzGcQB8SFgxm-M?usp=sharing\n",
    "\n",
    "- Leave-one-out\n",
    "    - There could be two ways to approach this:\n",
    "        - Interpret the gold standard:\n",
    "            - Iterate over kps in key_points_dev.csv and grab the respective argument from arguments_dev.csv\n",
    "        - Let the model decide what is the right arg\n",
    "            - Iterate over all args, computer score with each kp and save argmax\n",
    "    - What needs to be added is for each InputExample save the word we dropped so we can access it during\n",
    "      entailment computing. For this copy InputExample from SentenceBert and modify class, save word to later access\n",
    "\n",
    "- SHAP\n",
    "    - TODO\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import *\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import sys\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "sys.path.insert(0, \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/code/src-py\")\n",
    "import sbert_training\n",
    "work_tokenizer = word_tokenize\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "repo_dir = \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 10:27:12 - Use pytorch device: cuda\n",
      "2022-01-20 10:27:12 - Read Triplet train dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5702f17180942e3a90fb60e493f02e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Iteration:   0%|          | 0/645 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "623d2961a2044edcb42d8046dd6639f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-20 10:28:04 - TripletEvaluator: Evaluating the model on dev dataset in epoch 0 after 500 steps:\n",
      "mAP strict= 0.7961684424588478 ; mAP relaxed = 0.9755975144007354\n",
      "2022-01-20 10:28:05 - mAP strict:   \t79.62\n",
      "2022-01-20 10:28:05 - mAP relaxed:   \t97.56\n",
      "2022-01-20 10:28:05 - Save model to /home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-modelsroberta-base-contrastive-10-epochs-2022-01-20_10-27-03\n",
      "2022-01-20 10:28:20 - TripletEvaluator: Evaluating the model on dev dataset after epoch 0:\n",
      "mAP strict= 0.7999906478419604 ; mAP relaxed = 0.977392616323908\n",
      "2022-01-20 10:28:21 - mAP strict:   \t80.00\n",
      "2022-01-20 10:28:21 - mAP relaxed:   \t97.74\n",
      "2022-01-20 10:28:21 - Save model to /home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-modelsroberta-base-contrastive-10-epochs-2022-01-20_10-27-03\n"
     ]
    }
   ],
   "source": [
    "model = sbert_training.train_model('/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/siamese-data/',\n",
    "                                   \"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code-2/data/kpm_data\",\n",
    "                                   'dev',\n",
    "                                   \"/home/marcelbraasch/PycharmProjects/new_KPA/argmining-21-keypoint-analysis-sharedtask-code-2/code/siamese-models\",\n",
    "                                   'roberta-base',\n",
    "                                   model_suffix='contrastive-10-epochs',\n",
    "                                   data_file_suffix='contrastive',\n",
    "                                   num_epochs=1, max_seq_length=70, add_special_token=True, train_batch_size=32, loss='ContrastiveLoss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    max_seq_length = 70\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    word_embedding_model.max_seq_length = max_seq_length\n",
    "    word_embedding_model.tokenizer.add_tokens(['<SEP>'], special_tokens=True)\n",
    "    word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                                   pooling_mode_mean_tokens=True,\n",
    "                                   pooling_mode_cls_token=False,\n",
    "                                   pooling_mode_max_tokens=False)\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    return model\n",
    "\n",
    "# config = BertConfig.from_json_file(\"/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model/config.json\")\n",
    "# model = TFBertModel.from_pretrained('/home/marcelbraasch/PycharmProjects/argmining-21-keypoint-analysis-sharedtask-code/model/my_pytorch_model.bin', from_pt=True, config=config)\n",
    "\n",
    "path = \"./siamese_state_dict_2.pt\"\n",
    "state_dict = torch.load(path)[\"state_dict\"]\n",
    "model = load_model()\n",
    "model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_7246/2643918340.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m \u001B[0mload_kpm_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_7246/2643918340.py\u001B[0m in \u001B[0;36mload_kpm_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0;31m# Compute model scores\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m         \u001B[0;32mdef\u001B[0m \u001B[0mcompute_score_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m             \u001B[0margument\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"argument\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m             \u001B[0mkey_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"key_point\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_7246/2643918340.py\u001B[0m in \u001B[0;36mload_kpm_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0;31m# Compute model scores\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m         \u001B[0;32mdef\u001B[0m \u001B[0mcompute_score_from\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m             \u001B[0margument\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"argument\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m             \u001B[0mkey_point\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"key_point\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mtrace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m                 \u001B[0;31m# if thread has a suspend flag, we suspend with a busy wait\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    746\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0minfo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpydev_state\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mSTATE_SUSPEND\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 747\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    748\u001B[0m                     \u001B[0;31m# No need to reset frame.f_trace to keep the same trace function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    749\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrace_dispatch\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdo_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[0;31m# IFDEF CYTHON\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36mdo_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_threads_suspended_single_notification\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnotify_thread_suspended\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_reason\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1147\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1148\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1149\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_do_wait_suspend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthread\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mevent\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0marg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msuspend_type\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_this_thread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/213.5744.248/plugins/python/helpers/pydev/pydevd.py\u001B[0m in \u001B[0;36m_do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1160\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1161\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprocess_internal_commands\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1162\u001B[0;31m                 \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcancel_async_evaluation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mget_current_thread_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthread\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def create_arg_kps_mapping(arguments_df, key_points_df):\n",
    "    mapping = {}\n",
    "    topics = arguments_df[\"topic\"].unique()\n",
    "    for topic in topics:\n",
    "        arguments = arguments_df.loc[arguments_df[\"topic\"] == topic][[\"argument\"]].drop_duplicates()\n",
    "        key_points = key_points_df.loc[key_points_df[\"topic\"] == topic][[\"key_point\"]].drop_duplicates()\n",
    "        map = pd.merge(arguments, key_points, how=\"cross\")\n",
    "        mapping[topic] = map\n",
    "    return mapping\n",
    "\n",
    "def load_kpm_data():\n",
    "    data = {}\n",
    "    for subset in [\"dev\", \"train\"]:\n",
    "        # Load files\n",
    "        arguments_file = repo_dir + f\"data/kpm_data/arguments_{subset}.csv\"\n",
    "        key_points_file = repo_dir + f\"data/kpm_data/key_points_{subset}.csv\"\n",
    "        labels_file = repo_dir + f\"data/kpm_data/labels_{subset}.csv\"\n",
    "        arguments_df = pd.read_csv(arguments_file)\n",
    "        key_points_df = pd.read_csv(key_points_file)\n",
    "        labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "        # Get gold standard\n",
    "        positive_labels_df = labels_df.loc[labels_df[\"label\"] == 1]\n",
    "        gold_standard = pd.merge(positive_labels_df, key_points_df, how=\"inner\", on=\"key_point_id\")\n",
    "        gold_standard = pd.merge(gold_standard, arguments_df, how=\"inner\", on=\"arg_id\")\n",
    "        gold_standard = gold_standard[[\"argument\", \"key_point\"]]\n",
    "        data[subset] = gold_standard\n",
    "\n",
    "        # Compute model scores\n",
    "        def compute_score_from(row):\n",
    "            argument = row[\"argument\"]\n",
    "            key_point = row[\"key_point\"]\n",
    "            return compute_entailment_from_arg_kp(argument, key_point, model)\n",
    "\n",
    "        arg_to_kps = create_arg_kps_mapping(arguments_df, key_points_df)\n",
    "        for topic, arg_kps_mapping in arg_to_kps.items():\n",
    "            arg_kps_mapping['score'] = arg_kps_mapping.apply(lambda row: compute_score_from(row), axis=1)\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "load_kpm_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "\n",
    "    def __init__(self,\n",
    "                 arg: str = None,\n",
    "                 kp: str = None,\n",
    "                 label: Union[int, float] = 0,\n",
    "                 dropped_word: str = None\n",
    "                 ):\n",
    "\n",
    "        self.arg = arg\n",
    "        self.kp = kp\n",
    "        self.label = label\n",
    "        self.dropped_word = dropped_word\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<InputExample> label: {}, texts: {}\".format(str(self.label), \"; \".join(self.texts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_data_for_leave_one_out():\n",
    "    \"\"\"\n",
    "    Creates a nested list of data.\n",
    "    For each arg-kp pair we create no_of_words_in_kp InputExamples.\n",
    "    \"\"\"\n",
    "    dfs = load_kpm_data()\n",
    "    samples = []\n",
    "    # TODO: iterate of dfs and create examples\n",
    "    for _ in range(1):\n",
    "        topic = \"We should abandon the use of school uniform\"\n",
    "        arg = \"A real education is about giving students the tools to learn, think, and express themselves; dictating to them what to wear sends a strong message that we don't trust them to think on their own.\"\n",
    "        kp = \"School uniform is harming the student's self expression\"\n",
    "        label = 1\n",
    "        sample = [InputExample(arg=arg + \" <SEP> \" + topic,\n",
    "                               kp= kp ,#+ \" <SEP> \" + topic,\n",
    "                               label=label,\n",
    "                               dropped_word=\"Reference\")]\n",
    "\n",
    "        words = work_tokenizer(kp)\n",
    "\n",
    "\n",
    "        for i in range(len(words)):\n",
    "            new_kp = copy.deepcopy(words)\n",
    "            new_kp.pop(i)\n",
    "            new_kp_topic = \" \".join(new_kp)\n",
    "            dropped_word = words[i]\n",
    "            sample.append(InputExample(arg=arg,\n",
    "                                       kp=new_kp_topic,\n",
    "                                       label=label,\n",
    "                                       dropped_word=dropped_word))\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def compute_entailment(example, model):\n",
    "    arg = model.encode(example.arg, show_progress_bar=False),\n",
    "    kp = model.encode(example.kp, show_progress_bar=False)\n",
    "    return float(util.pytorch_cos_sim(arg, kp))\n",
    "\n",
    "def compute_entailment_from_arg_kp(arg, kp, model):\n",
    "    arg = model.encode(arg, show_progress_bar=False),\n",
    "    kp = model.encode(kp, show_progress_bar=False)\n",
    "    return float(util.pytorch_cos_sim(arg, kp))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropped_word': 'Reference', 'score': 0.635991632938385}\n",
      "{'dropped_word': 'School', 'score': 0.6456453800201416}\n",
      "{'dropped_word': 'uniform', 'score': 0.6269412636756897}\n",
      "{'dropped_word': 'is', 'score': 0.6195348501205444}\n",
      "{'dropped_word': 'harming', 'score': 0.551324725151062}\n",
      "{'dropped_word': 'the', 'score': 0.6279229521751404}\n",
      "{'dropped_word': 'student', 'score': 0.6066310405731201}\n",
      "{'dropped_word': \"'s\", 'score': 0.6129819750785828}\n",
      "{'dropped_word': 'self', 'score': 0.5350502729415894}\n",
      "{'dropped_word': 'expression', 'score': 0.5559728145599365}\n"
     ]
    }
   ],
   "source": [
    "def leave_one_out(model, data):\n",
    "    results = []\n",
    "    for sentences in samples:\n",
    "        result = []\n",
    "        for i, example in enumerate(sentences):\n",
    "            r = {\"dropped_word\": example.dropped_word,\n",
    "                 \"score\": compute_entailment(example, model)}\n",
    "            result.append(r)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "samples = get_data_for_leave_one_out()\n",
    "results = leave_one_out(model, samples)\n",
    "print(*(results[0]), sep=\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "############################################################\n",
    "############################################################\n",
    "################# CODE GRAVEYARD ###########################\n",
    "############################################################\n",
    "############################################################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# modified_features = [{\"input_ids\": features[0][\"input_ids\"],\n",
    "#                       \"attention_mask\": features[0][\"attention_mask\"]},\n",
    "#                      {\"input_ids\": features[1][\"input_ids\"],\n",
    "#                       \"attention_mask\": features[1][\"attention_mask\"]}]\n",
    "\n",
    "# Prepare tokens\n",
    "# kp_topic = modified_features[1][\"input_ids\"]\n",
    "# kp_topic_tokens = modified_features[1][\"input_ids\"].tolist()[0]\n",
    "# sep_token_index = kp_topic_tokens.index(50265) # <SEP> Token\n",
    "# kp_tokens = kp_topic_tokens[:sep_token_index]\n",
    "# topic_tokens = kp_topic_tokens[sep_token_index:]\n",
    "\n",
    "# Prepare list of kps where a token is dropped\n",
    "# new_kps = []\n",
    "# for i in range(len(kp_tokens)):\n",
    "#     dropped = kp_tokens[i]\n",
    "#     new_kp = copy.deepcopy(kp_tokens)\n",
    "#     new_kp.pop(i)\n",
    "#     new_kps.append({\"kp\": new_kp, \"dropped\": dropped, \"index\": i})\n",
    "\n",
    "# # Create dropped modified_features\n",
    "# for new_kp in new_kps:\n",
    "#     kp, dropped, index = new_kp.values()\n",
    "#\n",
    "#     # Modify sample itself\n",
    "#     modified_features[1][\"input_ids\"] = torch.tensor(kp+topic_tokens)\n",
    "#\n",
    "#     # Modify attention mask\n",
    "#     old_attention_mask = modified_features[1][\"attention_mask\"][0]\n",
    "#     first_part = old_attention_mask[:index]\n",
    "#     second_part = old_attention_mask[index+1:] # Here dropped index is excluded\n",
    "#     new_attention_mask = torch.cat([first_part, second_part], dim=0)\n",
    "#     new_attention_mask = new_attention_mask.view(1,len(new_attention_mask))\n",
    "#     modified_features[1][\"attention_mask\"] = new_attention_mask\n",
    "#\n",
    "#     dropped_score = compute_entailment(loss_model, modified_features, labels)\n",
    "#\n",
    "#     dropped_scores.append({\"score\": dropped_score,\n",
    "#                            \"dropped\": dropped,\n",
    "#                            \"index\": index})\n",
    "#     s = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# def get_backbone_model_and_tokenizer():\n",
    "#     name = 'roberta-base'\n",
    "#     backbone = models.Transformer(name)\n",
    "#     backbone.max_seq_length = 70\n",
    "#     backbone.tokenizer.add_tokens(['<SEP>'], special_tokens=True)\n",
    "#     backbone.auto_model.resize_token_embeddings(len(backbone.tokenizer))\n",
    "#     return backbone, backbone.tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# #Modify token embeddings\n",
    "# old_token_embedding = features[1][\"token_embeddings\"][0]\n",
    "# first_part = features[1][\"token_embeddings\"][0][:index]\n",
    "# second_part = features[1][\"token_embeddings\"][0][index+1:]\n",
    "# new_token_embedding = torch.cat([first_part, second_part])\n",
    "# new_token_embedding = new_token_embedding.view(1, *new_token_embedding.size())\n",
    "# features[1][\"token_embeddings\"] = new_token_embedding\n",
    "\n",
    "# Modify sentence embeddings\n",
    "# We can just drop the sentence embeddings, the model will infer this automatically"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# def get_dataloader(examples, model):\n",
    "#     dataloader = DataLoader(examples, shuffle=False, batch_size=1)\n",
    "#     dataloader.collate_fn = model.smart_batching_collate\n",
    "#     return iter(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# def get_dataloaders_for_leave_one_out(data, model):\n",
    "#     dataloaders = []\n",
    "#     for sample in data:\n",
    "#         # sample has no_of_words_in_kp examples in it\n",
    "#         dataloader = DataLoader(sample, shuffle=False, batch_size=1)\n",
    "#         #dataloader.collate_fn = model.smart_batching_collate\n",
    "#         dataloaders.append(dataloader)\n",
    "#     return dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# def prepare_model_for_inference(model):\n",
    "#     model = losses.ContrastiveLoss(model)\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "#     return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# def compute_entailment(model, features, labels):\n",
    "#     return float((1 - model(features, labels)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}